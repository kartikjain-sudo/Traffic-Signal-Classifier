{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    "from PIL import ImageTk, Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./model/Recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = { 1:'Speed limit (20km/h)',\n",
    "            2:'Speed limit (30km/h)',      \n",
    "            3:'Speed limit (50km/h)',       \n",
    "            4:'Speed limit (60km/h)',      \n",
    "            5:'Speed limit (70km/h)',    \n",
    "            6:'Speed limit (80km/h)',      \n",
    "            7:'End of speed limit (80km/h)',     \n",
    "            8:'Speed limit (100km/h)',    \n",
    "            9:'Speed limit (120km/h)',     \n",
    "           10:'No passing',   \n",
    "           11:'No passing veh over 3.5 tons',     \n",
    "           12:'Right-of-way at intersection',     \n",
    "           13:'Priority road',    \n",
    "           14:'Yield',     \n",
    "           15:'Stop',       \n",
    "           16:'No vehicles',       \n",
    "           17:'Veh > 3.5 tons prohibited',       \n",
    "           18:'No entry',       \n",
    "           19:'General caution',     \n",
    "           20:'Dangerous curve left',      \n",
    "           21:'Dangerous curve right',   \n",
    "           22:'Double curve',      \n",
    "           23:'Bumpy road',     \n",
    "           24:'Slippery road',       \n",
    "           25:'Road narrows on the right',  \n",
    "           26:'Road work',    \n",
    "           27:'Traffic signals',      \n",
    "           28:'Pedestrians',     \n",
    "           29:'Children crossing',     \n",
    "           30:'Bicycles crossing',       \n",
    "           31:'Beware of ice/snow',\n",
    "           32:'Wild animals crossing',      \n",
    "           33:'End speed + passing limits',      \n",
    "           34:'Turn right ahead',     \n",
    "           35:'Turn left ahead',       \n",
    "           36:'Ahead only',      \n",
    "           37:'Go straight or right',      \n",
    "           38:'Go straight or left',      \n",
    "           39:'Keep right',     \n",
    "           40:'Keep left',      \n",
    "           41:'Roundabout mandatory',     \n",
    "           42:'End of no passing',      \n",
    "           43:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(file_path):\n",
    "    global label_packed\n",
    "    image = Image.open(file_path)\n",
    "    image = image.resize((30,30))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = np.array(image)\n",
    "    pred = model.predict_classes([image])[0]\n",
    "    return classes[pred+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(path, x, y, w, h, ROI_number):\n",
    "    image = cv2.imread(path)\n",
    "    copy = image.copy()\n",
    "    \n",
    "    file_path = os.path.split(path)\n",
    "    ext = file_path[-1].split('.')\n",
    "#     print(ext)\n",
    "    \n",
    "    path_to_save = os.path.join(os.getcwd() + '\\\\result\\\\')\n",
    "#     print(path_to_save)\n",
    "\n",
    "    ROI = copy[y:y+h, x:x+w]\n",
    "    cv2.imwrite((os.path.join(path_to_save, ('ROI_{}.{}'.format(ROI_number,ext[1])))), ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(path):\n",
    "\n",
    "    image_BGR = cv2.imread(path)\n",
    "\n",
    "    print()\n",
    "    print('Image shape:', image_BGR.shape)  # tuple of (466, 700, 3)\n",
    "\n",
    "    # Getting spatial dimension of input image\n",
    "    h, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n",
    "\n",
    "    # Check point\n",
    "    # Showing height an width of image\n",
    "    print('Image height={0} and width={1}'.format(h, w))  # 466 700\n",
    "    \n",
    "    path_to_save = \"\"\n",
    "\n",
    "    # Getting blob from input image\n",
    "    # The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob\n",
    "    # from input image after mean subtraction, normalizing, and RB channels swapping\n",
    "    # Resulted shape has number of images, number of channels, width and height\n",
    "    # E.G.:\n",
    "    # blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n",
    "    blob = cv2.dnn.blobFromImage(image_BGR, 1 / 255.0, (416, 416),\n",
    "                                 swapRB=True, crop=False)\n",
    "\n",
    "    # Check point\n",
    "    # print('Blob shape:', blob.shape)  # (1, 3, 416, 416)\n",
    "\n",
    "    # Loading COCO class labels from file\n",
    "    # Opening file\n",
    "    \n",
    "    with open('Images/Detection/classes.names') as f:\n",
    "        # Getting labels reading every line\n",
    "        # and putting them into the list\n",
    "        labels = [line.strip() for line in f]\n",
    "\n",
    "    # Loading trained YOLO v3 Objects Detector\n",
    "    # with the help of 'dnn' library from OpenCV\n",
    "                                                      \n",
    "    network = cv2.dnn.readNetFromDarknet('config/yolov3_ts_test.cfg',\n",
    "                                         'weights/yolov3_ts.weights')\n",
    "\n",
    "    # Getting list with names of all layers from YOLO v3 network\n",
    "    layers_names_all = network.getLayerNames()\n",
    "\n",
    "    # Check point\n",
    "    # print()\n",
    "    # print(layers_names_all)\n",
    "\n",
    "    # Getting only output layers' names that we need from YOLO v3 algorithm\n",
    "    # with function that returns indexes of layers with unconnected outputs\n",
    "    layers_names_output = \\\n",
    "        [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
    "\n",
    "    # Check point\n",
    "    # print()\n",
    "    # print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "    # Setting minimum probability to eliminate weak predictions\n",
    "    probability_minimum = 0.5\n",
    "\n",
    "    # Setting threshold for filtering weak bounding boxes\n",
    "    # with non-maximum suppression\n",
    "    threshold = 0.3\n",
    "\n",
    "    # Generating colours for representing every detected object\n",
    "    # with function randint(low, high=None, size=None, dtype='l')\n",
    "    colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "    # Check point\n",
    "    # print()\n",
    "    # print(type(colours))  # <class 'numpy.ndarray'>\n",
    "    # print(colours.shape)  # (80, 3)\n",
    "    # print(colours[0])  # [172  10 127]\n",
    "\n",
    "\n",
    "    # Implementing forward pass with our blob and only through output layers\n",
    "    # Calculating at the same time, needed time for forward pass\n",
    "    network.setInput(blob)  # setting blob as input to the network\n",
    "    start = time.time()\n",
    "    output_from_network = network.forward(layers_names_output)\n",
    "    end = time.time()\n",
    "\n",
    "    # Showing spent time for forward pass\n",
    "    print()\n",
    "    print('Objects Detection took {:.5f} seconds'.format(end - start))\n",
    "\n",
    "\n",
    "    # Preparing lists for detected bounding boxes,\n",
    "    # obtained confidences and class's number\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "\n",
    "    # Going through all output layers after feed forward pass\n",
    "    for result in output_from_network:\n",
    "        # Going through all detections from current output layer\n",
    "        for detected_objects in result:\n",
    "            # Getting 80 classes' probabilities for current detected object\n",
    "            scores = detected_objects[5:]\n",
    "            # Getting index of the class with the maximum value of probability\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting value of probability for defined class\n",
    "            confidence_current = scores[class_current]\n",
    "\n",
    "            # Check point\n",
    "            # Every 'detected_objects' numpy array has first 4 numbers with\n",
    "            # bounding box coordinates and rest 80 with probabilities for every class\n",
    "            # print(detected_objects.shape)  # (85,)\n",
    "\n",
    "            # Eliminating weak predictions with minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial image size\n",
    "                # YOLO data format keeps coordinates for center of bounding box\n",
    "                # and its current width and height\n",
    "                # That is why we can just multiply them elementwise\n",
    "                # to the width and height\n",
    "                # of the original image and in this way get coordinates for center\n",
    "                # of bounding box, its width and height for original image\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Now, from YOLO data format, we can get top left corner coordinates\n",
    "                # that are x_min and y_min\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)\n",
    "\n",
    "    # Implementing non-maximum suppression of given bounding boxes\n",
    "    # With this technique we exclude some of bounding boxes if their\n",
    "    # corresponding confidences are low or there is another\n",
    "    # bounding box for this region with higher confidence\n",
    "\n",
    "    # It is needed to make sure that data type of the boxes is 'int'\n",
    "    # and data type of the confidences is 'float'\n",
    "    # https://github.com/opencv/opencv/issues/12789\n",
    "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,\n",
    "                               probability_minimum, threshold)\n",
    "\n",
    "\n",
    "    # Defining counter for detected objects\n",
    "    counter = 1\n",
    "\n",
    "    # Checking if there is at least one detected object after non-maximum suppression\n",
    "    if len(results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in results.flatten():\n",
    "            # Showing labels of the detected objects\n",
    "            print('Object {0}: {1}'.format(counter, labels[int(class_numbers[i])]))\n",
    "\n",
    "            # Getting current bounding box coordinates,\n",
    "            # its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "            \n",
    "            crop(path, x_min, y_min, box_width, box_height, counter)\n",
    "            \n",
    "            file_path = os.path.split(path)\n",
    "            ext = file_path[1].split('.')\n",
    "\n",
    "            path_to_save = os.path.join(os.getcwd() + '\\\\result\\\\')\n",
    "        \n",
    "            signal = recognize(os.path.join(path_to_save + 'ROI_{}.{}'.format(counter,ext[1]) ))\n",
    "\n",
    "            \n",
    "            # Incrementing counter\n",
    "            counter += 1\n",
    "\n",
    "            # Preparing colour for current bounding box\n",
    "            # and converting from numpy array to list\n",
    "            colour_box_current = colours[class_numbers[i]].tolist()\n",
    "\n",
    "            # # # Check point\n",
    "            # print(type(colour_box_current))  # <class 'list'>\n",
    "            # print(colour_box_current)  # [172 , 10, 127]\n",
    "\n",
    "            # Drawing bounding box on the original image\n",
    "            cv2.rectangle(image_BGR, (x_min, y_min),\n",
    "                          (x_min + box_width, y_min + box_height),\n",
    "                          colour_box_current, 2)\n",
    "\n",
    "            # Preparing text with label and confidence for current bounding box\n",
    "            text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n",
    "                                                   confidences[i])\n",
    "\n",
    "            # Putting text with label and confidence on the original image\n",
    "            cv2.putText(image_BGR, signal, (x_min, y_min - 5),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 0.7, colour_box_current, 2)\n",
    "\n",
    "    # Comparing how many objects where before non-maximum suppression\n",
    "    # and left after\n",
    "    print()\n",
    "    print('Total objects been detected:', len(bounding_boxes))\n",
    "    print('Number of objects left after non-maximum suppression:', counter - 1)\n",
    "    \n",
    "    if (counter > 1):\n",
    "        # Saving resulted image in jpg format by OpenCV function\n",
    "        # that uses extension to choose format to save with\n",
    "        cv2.imwrite((os.path.join(path_to_save, ('result_{}.{}'.format(ext[0],ext[1])))), image_BGR)\n",
    "        upload_image((os.path.join(path_to_save, ('result_{}.{}'.format(ext[0],ext[1])))))\n",
    "        \n",
    "    sign = '{} Traffic Sign Found'.format(counter-1)\n",
    "    \n",
    "    label.configure(foreground='#011638', text=sign)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_transparency(im, bg_colour=(255, 255, 255)):\n",
    "\n",
    "    if im.mode in ('RGBA', 'LA') or (im.mode == 'P' and 'transparency' in im.info):\n",
    "\n",
    "        alpha = im.convert('RGBA').split()[-1]\n",
    "\n",
    "        bg = Image.new(\"RGBA\", im.size, bg_colour + (255,))\n",
    "        bg.paste(im, mask=alpha)\n",
    "        return bg\n",
    "\n",
    "    else:\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_classify_button(file_path):\n",
    "    classify_b=Button(top,text=\"Classify Image\",command=lambda: classify(file_path),padx=10,pady=5)\n",
    "    classify_b.configure(background='#364156', foreground='white',font=('arial',10,'bold'))\n",
    "    classify_b.place(relx=0.79,rely=0.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=tk.Tk()\n",
    "top.geometry('1800x1600')\n",
    "top.title('Traffic sign classification')\n",
    "top.configure(background='#CDCDCD')\n",
    "\n",
    "label=Label(top,background='#CDCDCD', font=('arial',15,'bold'))\n",
    "sign_image = Label(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image(file_path = \"\"):\n",
    "    try:\n",
    "        if (file_path == \"\"):\n",
    "            file_path=filedialog.askopenfilename()\n",
    "        uploaded=Image.open(file_path)\n",
    "        uploaded.thumbnail(((top.winfo_width()/1.5),(top.winfo_height()/1.5)))\n",
    "        im=ImageTk.PhotoImage(uploaded)\n",
    "        \n",
    "        sign_image.configure(image=im)\n",
    "        sign_image.image=im\n",
    "        label.configure(text='')\n",
    "        show_classify_button(file_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "upload=Button(top,text=\"Upload an image\",command=upload_image,padx=10,pady=5)\n",
    "upload.configure(background='#364156', foreground='white',font=('arial',10,'bold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image shape: (183, 275, 3)\n",
      "Image height=183 and width=275\n",
      "\n",
      "Objects Detection took 0.75500 seconds\n",
      "\n",
      "Total objects been detected: 0\n",
      "Number of objects left after non-maximum suppression: 0\n",
      "\n",
      "Image shape: (1080, 1920, 3)\n",
      "Image height=1080 and width=1920\n",
      "\n",
      "Objects Detection took 0.72300 seconds\n",
      "Object 1: prohibitory\n",
      "WARNING:tensorflow:From <ipython-input-4-f475babdf2d0>:7: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "Object 2: danger\n",
      "\n",
      "Total objects been detected: 4\n",
      "Number of objects left after non-maximum suppression: 2\n",
      "\n",
      "Image shape: (259, 194, 3)\n",
      "Image height=259 and width=194\n",
      "\n",
      "Objects Detection took 0.71997 seconds\n",
      "\n",
      "Total objects been detected: 0\n",
      "Number of objects left after non-maximum suppression: 0\n",
      "\n",
      "Image shape: (174, 289, 3)\n",
      "Image height=174 and width=289\n",
      "\n",
      "Objects Detection took 0.61000 seconds\n",
      "\n",
      "Total objects been detected: 0\n",
      "Number of objects left after non-maximum suppression: 0\n"
     ]
    }
   ],
   "source": [
    "upload.pack(side=BOTTOM,pady=50)\n",
    "sign_image.pack(side=BOTTOM,expand=True)\n",
    "label.pack(side=BOTTOM,expand=True)\n",
    "heading = Label(top, text=\"Know Your Traffic Sign\",pady=20, font=('arial',20,'bold'))\n",
    "heading.configure(background='#CDCDCD',foreground='#364156')\n",
    "heading.pack()\n",
    "top.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
